{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misalkan terdapat data aktual (y) & data hasil prediksi model (yp)\n",
    "y = ['Yes', 'No', 'Yes', 'No', 'Yes', 'No']\n",
    "yp = ['Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 1. Create confusion matrix manually\n",
    "\n",
    "- | __pred \"Yes\"__ | __pred \"No\"__\n",
    "- | - | -\n",
    "__aktual \"Yes\"__ | __*3*__ | __*0*__ \n",
    "__aktual \"No\"__ | __*2*__ | __*1*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 2. Create confusion matrix using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred YES</th>\n",
       "      <th>Pred NO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual YES</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual NO</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Pred YES  Pred NO\n",
       "Actual YES         3        0\n",
       "Actual NO          2        1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "x = confusion_matrix(y, yp, labels=[\"Yes\", 'No'])\n",
    "df = pd.DataFrame(\n",
    "    x, columns=['Pred YES', 'Pred NO'], index=['Actual YES', 'Actual NO'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### True, False, Positif, Negatif\n",
    "\n",
    "- __True Positives (TP)__: Prediksi YES, Aktual YES.\n",
    "- __True Negatives (TN)__: Prediksi NO, Aktual NO.\n",
    "- __False Positives (FP)__: Prediksi YES, Aktual NO.\n",
    "- __False Negatives (FN)__: Prediksi NO, Aktual YES.\n",
    "\n",
    "\n",
    "- | __pred \"Yes\"__ | __pred \"No\"__\n",
    "- | - | -\n",
    "__aktual \"Yes\"__ | __*TP = 3*__ | __*FN = 0*__ \n",
    "__aktual \"No\"__ | __*FP = 2*__ | __*TN = 1*__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 0, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show TN, FP, FN, TP from confusion matrix \n",
    "tn, fp, fn, tp = confusion_matrix(y, yp).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 3. Evaluation Metrics from Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Accuracy__: Overall, how often is the classifier correct?\n",
    "    \n",
    "    ```bash\n",
    "    (TP + TN) / total_data = (3 + 1)/6 = 0.67\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __Misclassification Rate / Error Rate__: Overall, how often is it wrong?\n",
    "\n",
    "    ```bash\n",
    "    (FP + FN) / total_data = (2 + 0)/6 = 0.33\n",
    "    \n",
    "    or\n",
    "    \n",
    "    1 - Accuracy = 1 - 0.67 = 0.33\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __TP Rate / Sensitivity / Recall (positif)__: When it's actually yes, how often does it predict yes?\n",
    "\n",
    "    ```bash\n",
    "    TP / total_actual_YES = TP / (TP + FN) = 3/3 = 1 = 100%\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __FP Rate__: When it's actually no, how often does it predict yes?\n",
    "\n",
    "    ```bash\n",
    "    FP / total_actual_NO = FP / (FP + TN) = 2/3 = 0.67\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __TN Rate / Specificity / Selectivity / Recall (negatif)__: When it's actually no, how often does it predict no?\n",
    "\n",
    "    ```bash\n",
    "    TN / total_actual_NO = TN / (FP + TN) = 1/3 = 0.33\n",
    "    \n",
    "    or\n",
    "    \n",
    "    1 - False Positive Rate\n",
    "    ```\n",
    "     \n",
    "    \n",
    "- __FN Rate__: When it's actually yes, how often does it predict no?\n",
    "\n",
    "    ```bash\n",
    "    FN / total_actual_YES = FN / (TP + FN) = 0/3 = ~\n",
    "    \n",
    "    or\n",
    "    \n",
    "    1 - True Positive Rate\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __Precision(+)__: When it predicts yes, how often is it correct?\n",
    "\n",
    "    ```bash\n",
    "    TP / total_predict_YES = TP / (TP + FP) = 3/5 = 0.6\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __Precision(-)__: When it predicts no, how often is it correct?\n",
    "\n",
    "    ```bash\n",
    "    TN / total_predict_NO = TN / (TN + FN) = 1/1 = 1\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __Prevalence__: How often does the YES condition actually occur in our sample?\n",
    "\n",
    "    ```bash\n",
    "    actual_YES / total_data = (TP + FN) / total_data = 3/6 = 0.5 = 50%\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __Null Error Rate__: How often does the NO condition actually occur in our sample? How often you would be wrong if you always predicted the majority class. In our example, the null error rate would be: \n",
    "    \n",
    "    ```bash\n",
    "    aktual_NO / total_data = (FP + TN) / total_data = 3/6 = 0.5 \n",
    "    \n",
    "    *Coz if you always predict YES, you would only be wrong for the 3 \"NO\" cases!\n",
    "    \n",
    "    1 - Prevalence\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __F1 Score__: weighted average of the true positive rate (recall) and precision. The F1 score is the harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n",
    "    \n",
    "    ```bash\n",
    "    F1 Score = 2 * ((precision(+) * recall(+)) / (precision(+) + recall(+)))\n",
    "\n",
    "             = 2 * ((0.6 * 1) / (0.6 + 1)) = 2 * 0.375 = 0.75\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __Balanced Accuracy__: Accuracy is not a good metric for imbalanced data sets. For example, if you have 95 negative and 5 positive samples, classifying all as negative gives 0.95 accuracy score. Balanced Accuracy[9] (bACC) overcomes this problem, by normalizing true positive and true negative predictions by the number of positive and negative samples, respectively, and divides their sum into two.\n",
    "\n",
    "    ```bash\n",
    "    Balanced Acc = recallP (TP rate) + recallN (TN rate) / 2 = 1 + 0.3 / 2 = 0.66 \n",
    "    ```\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi = 0.6666666666666666\n",
      "Error Rate = 0.3333333333333333\n",
      "Recall(+) =  1.0\n",
      "FP Rate = 0.6666666666666666\n",
      "Recall(-) = 0.3333333333333333\n",
      "FN Rate = 0.0\n",
      "Precision(+) = 0.6\n",
      "Precision(-) = 1.0\n",
      "Prevalence = 0.5\n",
      "Null Error Rate = 0.5\n",
      "F1score = 0.7499999999999999\n",
      "Balanced acc = 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# akurasi = (TP + TN) / total_data\n",
    "print('Akurasi =', (tp + tn) / (len(y)))\n",
    "\n",
    "# Misclassification Rate / Error Rate = (FP + FN) / total_data\n",
    "print('Error Rate =', (fp + fn) / (len(y)))\n",
    "\n",
    "# TP Rate / Sensitivity / Recall (positif) = TP / total_actual_YES = TP / (TP + FN)\n",
    "print('Recall(+) = ', tp / (tp + fn))\n",
    "\n",
    "# FP Rate = FP / total_actual_NO = FP / (FP + TN)\n",
    "print('FP Rate =', fp / (fp + tn))\n",
    "\n",
    "# TN Rate / Specificity / Recall (negatif) = TN / total_actual_NO = TN / (FP + TN)\n",
    "print('Recall(-) =', tn / (fp + tn))\n",
    "\n",
    "# FN Rate = FN / total_actual_YES = FN / (TP + FN) = 0/3 = ~\n",
    "print('FN Rate =', fn / (tp + fn))\n",
    "\n",
    "# Precision(+) = TP / total_predict_YES = TP / (TP + FP)\n",
    "print('Precision(+) =', tp / (tp + fp))\n",
    "\n",
    "# Precision(-) = TN / total_predict_NO = TN / (TN + FN)\n",
    "print('Precision(-) =', tn / (tn + fn))\n",
    "\n",
    "# Prevalence = actual_YES / total_data = (TP + FN) / total_data\n",
    "print('Prevalence =', (tp + fn) / len(y))\n",
    "\n",
    "# Null Error Rate = aktual_NO / total_data = (FP + TN) / total_data\n",
    "print('Null Error Rate =', (fp + tn) / len(y))\n",
    "\n",
    "# F1 score = 2 * ((precision * recall) / (precision + recall))\n",
    "print(\n",
    "    'F1score =', 2 * ( ((tp / (tp + fp)) * (tp / (tp + fn))) / ((tp / (tp + fp)) + (tp / (tp + fn))) )\n",
    ")\n",
    "\n",
    "# Balanced accuracy = recall(+) + recall(-) / 2\n",
    "print('Balanced acc =', ((tp / (tp + fn)) + (tn / (fp + tn))) / 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 4. Evaluation Metrics from Confusion Matrix with Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### __Accuracy & Misclassification Rate/Error Rate__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi = 0.6666666666666666\n",
      "Error Rate = 0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Akurasi =', accuracy_score(y, yp))\n",
    "\n",
    "# same as accuracy score\n",
    "# print('Akurasi =', model.score(y, yp))\n",
    "\n",
    "print('Error Rate =', 1 - accuracy_score(y, yp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### __Recall__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall(+) = 1.0\n",
      "Recall(-) = 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print('Recall(+) =', recall_score(y, yp, pos_label='Yes')) # TP Rate / Recall positif\n",
    "print('Recall(-) =', recall_score(y, yp, pos_label='No'))  # TN Rate / Recall Negatif\n",
    "# pos_label = positif label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### __Precision__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision(+) = 0.6\n",
      "Precision(-) = 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "print('Precision(+) =', precision_score(y, yp, pos_label='Yes')) # Precision positif\n",
    "print('Precision(-) =', precision_score(y, yp, pos_label='No'))  # Precision Negatif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### __F1 Score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (+) = 0.7499999999999999\n",
      "F1 Score (-) = 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('F1 Score (+) =', f1_score(y, yp, pos_label='Yes'))\n",
    "# 2 * ((precision(+) * recall(+)) / (precision(+) + recall(+)))\n",
    "\n",
    "print('F1 Score (-) =', f1_score(y, yp, pos_label='No'))\n",
    "# 2 * ((precision(-) * recall(-)) / (precision(-) + recall(-)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### __Balanced accuracy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced acc = 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "print('Balanced acc =', balanced_accuracy_score(y, yp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 5. Evaluation Metrics from precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1. , 0.6]), array([0.33333333, 1.        ]), array([0.5 , 0.75]), array([3, 3], dtype=int64))\n",
      "\n",
      "Precision [\"No\", \"Yes\"] = [1.  0.6]\n",
      "Recall    [\"No\", \"Yes\"] = [0.33333333 1.        ]\n",
      "Fscore    [\"No\", \"Yes\"] = [0.5  0.75]\n",
      "Support   [\"No\", \"Yes\"] = [3 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, fscore, support = precision_recall_fscore_support(y, yp)\n",
    "# SUPPORT is the number of occurrences of each class in y_true\n",
    "\n",
    "print(precision_recall_fscore_support(y, yp))\n",
    "print()\n",
    "print('Precision [\"No\", \"Yes\"] =', precision)\n",
    "print('Recall    [\"No\", \"Yes\"] =', recall)\n",
    "print('Fscore    [\"No\", \"Yes\"] =', fscore)\n",
    "print('Support   [\"No\", \"Yes\"] =', support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6666666666666666, 0.6666666666666666, 0.6666666666666666, None)\n",
      "(0.8, 0.6666666666666666, 0.625, None)\n",
      "(0.7999999999999999, 0.6666666666666666, 0.6249999999999999, None)\n"
     ]
    }
   ],
   "source": [
    "print(precision_recall_fscore_support(y, yp, average='micro'))\n",
    "print(precision_recall_fscore_support(y, yp, average='macro'))\n",
    "print(precision_recall_fscore_support(y, yp, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "0.8\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# Micro average vs macro average vs weighted average\n",
    "# tp, fp, fn, tn\n",
    "# (3, 2, 0, 1)\n",
    "\n",
    "precisionP = tp / (tp + fp)\n",
    "precisionN = tn / (tn + fn)\n",
    "\n",
    "preMicro = (tp + tn) / (tp + fp + tn + fn)\n",
    "preMacro = (precisionP + precisionN) / 2\n",
    "preWeighted = ((1 * precisionP) + (1 * precisionN)) / (1 + 1)\n",
    "\n",
    "print(preMicro)\n",
    "print(preMacro)\n",
    "print(preWeighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 6. Evaluation Metrics from Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       1.00      0.33      0.50         3\n",
      "         Yes       0.60      1.00      0.75         3\n",
      "\n",
      "    accuracy                           0.67         6\n",
      "   macro avg       0.80      0.67      0.62         6\n",
      "weighted avg       0.80      0.67      0.62         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y, yp))\n",
    "# SUPPORT is the number of occurrences of each class in y_true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
