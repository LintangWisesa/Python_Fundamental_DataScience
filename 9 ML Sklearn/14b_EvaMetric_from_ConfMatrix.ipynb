{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misalkan terdapat data aktual (y) & data hasil prediksi model (yp)\n",
    "y = ['Yes', 'No', 'Yes', 'No', 'Yes', 'No']\n",
    "yp = ['Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 1. Create confusion matrix manually\n",
    "\n",
    "- | __pred \"Yes\"__ | __pred \"No\"__\n",
    "- | - | -\n",
    "__aktual \"Yes\"__ | __*3*__ | __*0*__ \n",
    "__aktual \"No\"__ | __*2*__ | __*1*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 2. Create confusion matrix using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred YES</th>\n",
       "      <th>Pred NO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual YES</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual NO</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Pred YES  Pred NO\n",
       "Actual YES         3        0\n",
       "Actual NO          2        1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "x = confusion_matrix(y, yp, labels=[\"Yes\", 'No'])\n",
    "df = pd.DataFrame(\n",
    "    x, columns=['Pred YES', 'Pred NO'], index=['Actual YES', 'Actual NO'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### True, False, Positif, Negatif\n",
    "\n",
    "- __True Positives (TP)__: Prediksi YES, Aktual YES.\n",
    "- __True Negatives (TN)__: Prediksi NO, Aktual NO.\n",
    "- __False Positives (FP)__: Prediksi YES, Aktual NO.\n",
    "- __False Negatives (FN)__: Prediksi NO, Aktual YES.\n",
    "\n",
    "\n",
    "- | __pred \"Yes\"__ | __pred \"No\"__\n",
    "- | - | -\n",
    "__aktual \"Yes\"__ | __*TP = 3*__ | __*FN = 0*__ \n",
    "__aktual \"No\"__ | __*FP = 2*__ | __*TN = 1*__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 0, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show TN, FP, FN, TP from confusion matrix \n",
    "tn, fp, fn, tp = confusion_matrix(y, yp).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 3. Evaluation Metrics from Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Accuracy__: Overall, how often is the classifier correct?\n",
    "    \n",
    "    ```bash\n",
    "    (TP + TN) / total_data = (3 + 1)/6 = 0.67\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __Misclassification Rate / Error Rate__: Overall, how often is it wrong?\n",
    "\n",
    "    ```bash\n",
    "    (FP + FN) / total_data = (2 + 0)/6 = 0.33\n",
    "    \n",
    "    or\n",
    "    \n",
    "    1 - Accuracy = 1 - 0.67 = 0.33\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __TP Rate / Sensitivity / Recall (positif)__: When it's actually yes, how often does it predict yes?\n",
    "\n",
    "    ```bash\n",
    "    TP / total_actual_YES = 3/3 = 1 = 100%\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __FP Rate__: When it's actually no, how often does it predict yes?\n",
    "\n",
    "    ```bash\n",
    "    FP / total_actual_NO = 2/3 = 0.67\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __TN Rate / Recall (negatif)__: When it's actually no, how often does it predict no?\n",
    "\n",
    "    ```bash\n",
    "    TN / total_actual_NO = 1/3 = 0.33\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __FN Rate__: When it's actually yes, how often does it predict no?\n",
    "\n",
    "    ```bash\n",
    "    FN / total_actual_YES = 0/3 = ~\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __Precision__: When it predicts yes, how often is it correct?\n",
    "\n",
    "    ```bash\n",
    "    TP / total_predict_YES = 3/5 = 0.6\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __Prevalence__: How often does the YES condition actually occur in our sample?\n",
    "\n",
    "    ```bash\n",
    "    actual_YES / total_data = 3/6 = 0.5 = 50%\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __F1 Score__: weighted average of the true positive rate (recall) and precision. The F1 score is the harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n",
    "    \n",
    "    ```bash\n",
    "    F1 Score = 2 * ((precision * recall) / (precision + recall))\n",
    "\n",
    "             = 2 * ((0.6 * 1) / (0.6 + 1)) = 2 * 0.375 = 0.75\n",
    "    ```\n",
    "    \n",
    "    \n",
    "- __Null Error Rate__: How often you would be wrong if you always predicted the majority class. In our example, the null error rate would be: \n",
    "    \n",
    "    ```bash\n",
    "    aktual_NO / total_data = 3/6 = 0.5 \n",
    "    \n",
    "    *Coz if you always predict YES, you would only be wrong for the 3 \"NO\" cases!\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 4. Evaluation Metrics from Confusion Matrix with Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### __Accuracy & Misclassification Rate/Error Rate__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi = 0.6666666666666666\n",
      "Error Rate = 0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Akurasi =', accuracy_score(y, yp))\n",
    "print('Error Rate =', 1 - accuracy_score(y, yp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### __Recall__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall(+) = 1.0\n",
      "Recall(-) = 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print('Recall(+) =', recall_score(y, yp, pos_label='Yes')) # TP Rate / Recall positif\n",
    "print('Recall(-) =', recall_score(y, yp, pos_label='No'))  # TN Rate / Recall Negatif\n",
    "# pos_label = positif label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### __Precision__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision(+) = 0.6\n",
      "Precision(-) = 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "print('Precision(+) =', precision_score(y, yp, pos_label='Yes')) # Precision positif\n",
    "print('Precision(-) =', precision_score(y, yp, pos_label='No'))  # Precision Negatif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### __F1 Score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (+) = 0.7499999999999999\n",
      "F1 Score (-) = 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('F1 Score (+) =', f1_score(y, yp, pos_label='Yes'))\n",
    "# 2 * ((precision(+) * recall(+)) / (precision(+) + recall(+)))\n",
    "\n",
    "print('F1 Score (-) =', f1_score(y, yp, pos_label='No'))\n",
    "# 2 * ((precision(-) * recall(-)) / (precision(-) + recall(-)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
